{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert images and text in CLIP embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure in your directory are present hmc_info.csv and a folder named HMC with all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = 'HMC/'                         # Folder containing the images\n",
    "num_images = len(os.listdir(img_folder))\n",
    "print(\"Number of elements in the image folder:\", num_images)\n",
    "\n",
    "jsonl_file = os.path.join(os.getcwd(), 'hmc_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, jsonl_file, img_folder=None, model_name=\"ViT-L/14\", device='cpu', load_text=True, load_images=True, split='train'):\n",
    "        self.device = device\n",
    "        self.load_text = load_text\n",
    "        self.load_images = load_images\n",
    "        self.model_name = model_name\n",
    "        self.data = self._load_data(jsonl_file, split)\n",
    "        self.split = split\n",
    "\n",
    "        # Load model\n",
    "        if self.load_text or self.load_images:\n",
    "            self.clip_model = clip.load(model_name, device=device)[0]  \n",
    "            self.clip_processor = clip.tokenize\n",
    "        \n",
    "        if self.load_images:\n",
    "            assert img_folder is not None, \"Image folder must be provided to load images.\"\n",
    "            self.img_folder = img_folder\n",
    "            self.image_size = 224\n",
    "            self.image_transform = Resize((self.image_size, self.image_size))  \n",
    "            self.to_tensor = ToTensor()  \n",
    "\n",
    "    def _load_data(self, file_path, split):\n",
    "        \n",
    "        \"\"\"\n",
    "        Loads data from a CSV file.\n",
    "        \"\"\"\n",
    "        data = []\n",
    "        with open(file_path, 'r', encoding = 'utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            for row in reader:\n",
    "                if row['split'] == split:\n",
    "                    data.append(row)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_data = self.data[idx]\n",
    "        result = item_data.copy() \n",
    "        \n",
    "        if self.load_text:\n",
    "            if len(item_data['text'])>250:\n",
    "                item_data['text'] = item_data['text'][:250]\n",
    "                #print(1)\n",
    "            text_tokens = self.clip_processor([item_data['text']]).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                text_features = self.clip_model.encode_text(text_tokens)\n",
    "            result['text_embedding'] = text_features.cpu()\n",
    "\n",
    "        if self.load_images and 'img' in item_data:\n",
    "            image_path = os.path.join(self.img_folder, item_data['img'].replace('img/', ''))\n",
    "            if os.path.exists(image_path):\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = self.image_transform(image)  # Apply resizing\n",
    "                image = self.to_tensor(image)  # Convert image to tensor\n",
    "                image_inputs = image.unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    image_features = self.clip_model.encode_image(image_inputs)\n",
    "                result['image_embedding'] = image_features.cpu()\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method('spawn')\n",
    "\n",
    "splits = ['train', 'dev_seen','dev_unseen', 'test_seen', 'test_unseen']\n",
    "\n",
    "for split in splits:\n",
    "    dataset = MultimodalDataset(\n",
    "            jsonl_file=jsonl_file,\n",
    "            img_folder=img_folder,\n",
    "            device=device,\n",
    "            load_text= True,\n",
    "            load_images= True,\n",
    "            split=split\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, pin_memory=True, num_workers=4)\n",
    "    total_batches = len(dataloader)\n",
    "    progress_bar = tqdm(total=total_batches, desc='Processing Batches')\n",
    "\n",
    "    for batch in dataloader:\n",
    "        progress_bar.update()\n",
    "    progress_bar.close()\n",
    "    datalist=[]\n",
    "    for data in dataloader.dataset:\n",
    "            datalist.append(data)\n",
    "\n",
    "    # Remove unnecessary data\n",
    "    for d in datalist:\n",
    "        d.pop('text')\n",
    "        d.pop('img')\n",
    "        d['text'] = d['text_embedding'].to(dtype=torch.float32)\n",
    "        d['image'] = d['image_embedding'].to(dtype=torch.float32)\n",
    "        d.pop('text_embedding')\n",
    "        d.pop('image_embedding')\n",
    "        d.pop('split')\n",
    "\n",
    "    data_dir = 'data'\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    if not os.path.exists(f'{data_dir}/hateful_memes'):\n",
    "        os.makedirs(f'{data_dir}/hateful_memes')\n",
    "    torch.save(datalist, f'data/hateful_memes/{split}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
